\chapter{Application to Lie groups}\label{ch:ApplicationToLieGroups}
\minitoc
\hf Due to the classical results of Cartan, Iwasawa and others, we know that any connected Lie group \index{Lie group} $G$ is diffeomorphic to the product of a maximally compact subgroup $K$ and the Euclidean space. In particular, $G$ deforms to $K$. For semisimple groups, this decomposition is stronger and is attributed to Iwasawa. The Killing form \index{Killing form} on the Lie algebra \index{Lie algebra} $\mathfrak{g}$ is non-degenerate and negative definite for compact semi-simple Lie algebras. For such a Lie group $G$, consider the Levi-Civita connection \index{Levi-Civita connection} associated to the bi-invariant metric obtained from negative of the Killing form. This connection coincides with the Cartan connection. 

\bigskip
\hf We will consider two examples, both of which are non-compact and non semisimple. We prove that these Lie groups $G$ deformation retract to maximally compact subgroups $K$ via gradient flows of appropriate Morse-Bott functions.\index{Morse-Bott functions} This requires a choice of a left-invariant metric which is right-$K$-invariant, and a careful analysis of the geodesics associated with the metric. In particular, we provide a possibly new proof of the surjectivity of the exponential map for $U(p,q)$. The results of this chapter is based on joint work with Basu \cite[\S 4]{BaPr21}. 

\section{Matrices with positive determinant}\label{Sec: GLnSOn}

\hfb Let $g$ be a left-invariant metric on $GL(n,\rbb)$, the set of all invertible matrices. Recall that a left-invariant metric $g$ on a Lie group is determined by its restriction at the identity. For $A\in GL(n,\rbb)$, consider the left multiplication map $l_A:GL(n,\rbb) \to GL(n,\rbb),~ B\mapsto AB$. This extends to a linear isomorphism from $M(n,\rbb)$ to itself. Thus, the differential $(Dl_A)_I:T_IGL(n,\rbb)\to T_AGL(n,\rbb)$ is an isomorphism and given by $l_A$ itself. For $X,Y\in T_IGL(n,\rbb)$, 
	\begin{equation*} \label{eq: left-invariant iso}
		g_I(X,Y) = g_A((Dl_A)_IX,(Dl_A)_IY)=g_A(AX,AY).
	\end{equation*}
We choose the left-invariant metric on $GL(n,\rbb)$ generated by the Euclidean metric at $I$. Therefore,
\begin{displaymath}
    g_{A^{-1}}(X,Y) = \innerprod{AX}{AY}_I := \trace{(AX)^T\!AY} = \trace{X^T\!A^T\!AY}.
\end{displaymath}
Note that this metric is right-$O(n,\rbb)$-invariant. We are interested in the distance between an invertible matrix $A$ (with $\det(A)>0$) and $SO(n,\rbb)$. Since $SO(n,\rbb)$ is compact, there exists $B\in SO(n,\rbb) $ such that $d(A,B) = \dist(A,SO(n,\rbb))$. 
\begin{lemma}\label{CartanGLn}
    If $D$ is a diagonal matrix with positive diagonal entries $\lambda_1,\cdots,\lambda_n$, then 
    \begin{displaymath}
        \dist(D,SO(n,\rbb)) = d(D,I).
    \end{displaymath}
    Moreover, $I$ is the unique minimizer and the associated minimal geodesic is given by $\gamma(t)=e^{t\log D}$.
\end{lemma}
\begin{proof}
    Let $B\in SO(n,\rbb)$ satisfying $d(D,B) = \dist(D,SO(n,\rbb))$. Since with respect to the left-invariant metric $GL^+(n,\rbb)$ is complete, there exists a minimal geodesic $\gamma:[0,1]\to GL^+(n,\rbb)$ joining $B$ to $D$, i.e., 
    \begin{displaymath}
        \gamma(0) = B,~~\gamma(1) = D, ~~\text{ and } ~~ l(\gamma) = d(D,B).
    \end{displaymath}
    The first variational principle implies that $\gamma'(0)$ is orthogonal to $T_BSO(n,\rbb)$. It follows from \cite[\S 2.1]{MaNe16} that $\eta(t)=e^{tW}$ is a geodesic if $W$ is a symmetric matrix. Moreover, $\eta'(0)=W$ is orthogonal to $T_I SO(n,\rbb)$. As left translation is an isometry and isometry preserves geodesic, it follows that $\gamma(t) = Be^{tW}$ is a geodesic with $\gamma'(0)$ orthogonal to $T_BSO(n,\rbb)$. By the defining properties of $\gamma$,  $D=\gamma(1)=Be^W$. Since $e^W$ is symmetric positive definite, we obtain two polar decompositions \index{polar decompositions} of $D$, i.e., $D = ID$ and $ D = Be^W$. By the uniqueness of the polar decomposition for invertible matrices, $B=I$ and $D=e^W$. 

    \vspace{0.2cm}
    \hf In order to compute $d(I,D)$, note that 
    \begin{displaymath}
        e^W = D = e^{\log D},
    \end{displaymath}
    where $\log D$ denotes the diagonal matrix with entries $\log \lambda_1,\cdots,\log \lambda_n$. As $W$ and $\log D$ are symmetric, and matrix exponential is injective on the space of symmetric matrices, we conclude that $W = \log D$. The geodesic is given by $\gamma(t)=e^{t\log D}$ and 
    \begin{equation}\label{eq: distance left invariant}
        \dist(D,SO(n,\rbb)) = \norm{\gamma'(0)}_I = \norm{\log D}_I = \left(\sum_{i=1}^n (\log \lambda_i)^2\right)^{\frac{1}{2}}.
    \end{equation}
    Thus, the distance squared function will be given by $\sum_{i=1}^n (\log \lambda_i)^2$. 
\end{proof}

\hf Now for any $A\in GL^+(n,\rbb)$ we can apply the SVD decomposition, \index{SVD decomposition} i.e., $A = UDV^T$ with $\sqrt{A^TA} = VDV^T$ and $\log\sqrt{A^TA} = V(\log D) V^T$. Note that $U,V\in SO(n,\rbb)$ and $D$ is a diagonal matrix with positive entries. The left-invariant metric is right-invariant with respect to orthogonal matrices. Thus,
\begin{displaymath}
    \dist(A,SO(n,\rbb)) = \dist(D,SO(n,\rbb)) = \norm{\log D}_I,
\end{displaymath}
where the last equality follows from the lemma (see \eqref{eq: distance left invariant}). As 
\begin{displaymath}
    \norm{\log D}_I = \norm{V(\log D) V^T}_I = \norm{\log\sqrt{A^TA}}_I,
\end{displaymath}
it follows from the arguments of the lemma and the metric being bi-$O(n,\rbb)$-invariant that
\begin{displaymath}
    \gamma(t)=Ue^{t \log D}V^T
\end{displaymath}
is a minimal geodesic joining $UV^T$ to $A$, realizing $\dist(A,SO(n,\rbb))$. As the minimizer $UV^T$ is unique, $\mathrm{Se}(SO(n,\rbb))$ is empty, implying that $\mathrm{Cu}(SO(n,\rbb))$ is empty as well. In fact, $UV^T=A\sqrt{A^T A}^{-1}$ and 
\begin{equation}\label{GLdefOver2}
    \gamma(t)=Ue^{t \log D}V^T=UV^T Ve^{t \log D}V^T=A\sqrt{A^T A}^{-1}e^{t\log \sqrt{A^T A}}.
\end{equation}
If we compare \eqref{GLdefOver1}, the deformation of $GL(n,\rbb)$ to $O(n,\rbb)$ inside $M(n,\rbb)$, with \eqref{GLdefOver2}, then in both of the cases, an invertible matrix $A$ deforms to $A\sqrt{A^T A}^{-1}$. Finally, observe that the normal bundle of $SO(n,\rbb)$ is diffeomorphic to $GL^+(n,\rbb)$.
	

\section{Indefinite unitary groups}\label{Sec: Upq}
\index{indefinite unitary groups}	
\hfb Let $n$ be a positive integer with $n=p+q$. Consider the inner product on $\C^n$ given by
\begin{displaymath}
    \lan (w_1,\ldots,w_n),(z_1,\ldots,z_n)\ran = z_1\overline{w_1}+\cdots+z_p\overline{w_p}-z_{p+1}\overline{w_{p+1}}-\cdots- z_n\overline{w_n}.
\end{displaymath}
This is given by the matrix $I_{p,q}$ in the following way:
\begin{displaymath}
    \lan \mathbf{w},\mathbf{z}\ran=\overline{\mathbf{w}}^t I_{p,q} \mathbf{z}=\left(\begin{array}{ccc}
        \overline{w}_1 & \cdots & \overline{w}_n
        \end{array}\right)\left(\begin{array}{cc}
        I_p & 0 \\
        0 & -I_q
        \end{array}\right)\left(\begin{array}{c}
        z_1\\
        \vdots\\
        z_n
     \end{array}\right)
\end{displaymath}

\hf Let $U(p,q)$ denote the subgroup of $GL(n,\C)$ preserving this indefinite form, i.e., $\mathcal{A}\in \upq$ if and only if $\mathcal{A}^\ast I_{p,q}\mathcal{A}=I_{p,q}$. In particular, $\det \mathcal{A}$ is a complex number of unit length. By convention, $I_{n,0}=I_n$ and $ I_{0,n}=-I_n$, both of  which corresponds to $U(n,0)=U(n)=U(0,n)$, the unitary group. In all other cases, the inner product is indefinite.

\bigskip
\hf The group $U(1,1)$ is given by matrices of the form
\begin{displaymath}
    \mathcal{A}=\left(
        \begin{array}{cc}
            \alpha & \beta\\
            \lambda \overline{\beta} & \lambda \overline{\alpha}
        \end{array}\right),\,\,\lambda\in S^1,\,\,|\alpha|^2-|\beta|^2=1.
\end{displaymath}
More generally, we shall use 
\begin{displaymath}
    \mathcal{A}=\left(
        \begin{array}{cc}
            A & B\\
            C & D
        \end{array}\right)
\end{displaymath}
to denote an element of $\upq$. It follows from the definition that $\mathcal{A}\in \upq$ if and only if
\begin{eqnarray*}
    A^\ast A-C^\ast C& = & I_p\\
    A^\ast B-C^\ast D& = & 0_{p\times q}\\
    B^\ast B-D^\ast D& = & -I_q.
\end{eqnarray*}
Observe that if $Av=0$, then
\begin{displaymath}
    0=A^\ast Av=C^\ast Cv+v,
\end{displaymath}
which implies that $C^\ast C$, a positive semi-definite matrix, has $-1$ as an eigenvalue unless $v=0$. Therefore, $A$ is invertible, and the same argument works for $D$.
\begin{lemma}
    The intersection of $U(p+q)$ with $\upq$ is $U(p)\times U(q)$. Moreover, if $\mathcal{A}\in \upq$, then $\mathcal{A}^\ast,\sqrt{\mathcal{A}^\ast\mathcal{A}}\in \upq$.
\end{lemma}
\begin{proof}
    If $\mathcal{A}\in U(p)\times U(q)$, then 
    \begin{eqnarray*}
        A^\ast A+C^\ast C& = & I_p\\
        B^\ast B+D^\ast D& = & I_q.
    \end{eqnarray*}
    This implies that both $B$ and $C$ are zero matrices. If $\mathcal{A}\in \upq$, then $\mathcal{A}^\ast=I_{p,q}\mathcal{A}^{-1}I_{p,q}$ and
    \begin{align*}
        (\mathcal{A}^\ast\mathcal{A})^\ast I_{p,q}(\mathcal{A}^\ast\mathcal{A}) & =(\mathcal{A}^\ast\mathcal{A}) I_{p,q}(\mathcal{A}^\ast\mathcal{A}) \\
        & =I_{p,q}\mathcal{A}^{-1}I_{p,q}\mathcal{A}I_{p,q} I_{p,q}\mathcal{A}^{-1}I_{p,q}\mathcal{A}\\
        & =I_{p,q}=\mathcal{A}^\ast I_{p,q}\mathcal{A}.
    \end{align*}
    This also implies that $\mathcal{A}  I_{p,q}\mathcal{A}^\ast=I_{p,q}$. 

    \vspace{0.1cm}
    \hf All the eigenvalues of $\mathcal{A}^\ast\mathcal{A}$ are positive. Moreover, if $\lambda$ is an eigenvalue of $\mathcal{A}^\ast\mathcal{A}$ with eigenvector $\mathbf{v}=(v_1,\ldots,v_p,v_{p+1},\ldots, v_n)$, then 
    \begin{displaymath}
        I_{p,q}\mathbf{v}=\mathcal{A}^\ast\mathcal{A}\,I_{p,q}\mathcal{A}^\ast\mathcal{A}\mathbf{v}=\lambda(\mathcal{A}^\ast\mathcal{A}\,I_{p,q}\mathbf{v}),
    \end{displaymath}
    which implies that $\lambda^{-1}$ is also an eigenvalue with eigenvector $\mathbf{v}'=(v_1,\ldots,v_p,-v_{p+1},\ldots, -v_n)$. If $\{\mathbf{v}_1, \ldots,\mathbf{v}_n\}$ is an eigenbasis of $\mathcal{A}^\ast\mathcal{A}$ with (possibly repeated) eigenvalues $\lambda_1,\ldots,\lambda_n$, then 
    \begin{displaymath}
        \sqrt{\mathcal{A}^\ast\mathcal{A}}\,I_{p,q}\sqrt{\mathcal{A}^\ast\mathcal{A}}\mathbf{v}_j=\sqrt{\mathcal{A}^\ast\mathcal{A}}\,I_{p,q}\sqrt{\lambda_j}\mathbf{v}_j=\sqrt{\lambda_j}\sqrt{\mathcal{A}^\ast\mathcal{A}}\mathbf{v}_j'=\mathbf{v}_j'=I_{p,q}\mathbf{v}_j.
    \end{displaymath}
    Thus, $\sqrt{\mathcal{A}^\ast\mathcal{A}}$ satisfies the defining relation for a matrix to be in $\upq$. 
\end{proof}

\hf We may use the polar decomposition (for matrices in $GL(n,\C)$) to write
\begin{displaymath}
    \mathcal{A}=U |\mathcal{A}|,\,\,\textup{where}\,\,U=\mathcal{A}\left(\sqrt{\mathcal{A}^\ast\mathcal{A}}\right)^{-1}, |\mathcal{A}|=\sqrt{\mathcal{A}^\ast\mathcal{A}},
\end{displaymath}
where $U,|\mathcal{A}|\in \upq$. For $\textup{U}(1,1)$ this decomposition takes the form
\begin{displaymath}
    \left(\begin{array}{cc}
        \alpha & \beta\\
        \lambda \overline{\beta} & \lambda \overline{\alpha}
        \end{array}\right)=\left(\begin{array}{cc}
        \frac{\alpha}{|\alpha|} & 0\\
        0  & \lambda \frac{\overline{\alpha}}{|\alpha|}
        \end{array}\right)\left(\begin{array}{cc}
        |\alpha| & \frac{|\alpha|\beta}{\alpha}\\
        \frac{|\alpha| \overline{\beta}}{\overline{\alpha}} & |\alpha|
    \end{array}\right)
\end{displaymath}

\hf The Lie algebra $\mathfrak{u}_{p,q}$ is given by matrices $X\in M_n(\C)$ such that
\begin{displaymath}
    X^\ast I_{p,q}+I_{p,q}X=0.
\end{displaymath}
This is real Lie subalgebra of $M_{p+q}(\C)$. It contains the subalgebras $\mathfrak{u}_p, \mathfrak{u}_q$ as Lie algebras of the subgroups $U(p)\times I_q$ and $I_p \times U(q)$. Consider the inner product 
\begin{displaymath}
    \lan \cdot,\cdot\ran:\mathfrak{u}_{p,q}\times \mathfrak{u}_{p,q}\to\R,\,\,\,\lan X,Y\ran:=\textup{trace}(X^\ast Y).
\end{displaymath}

\begin{lemma}
    The inner product is symmetric and positive-definite. 
\end{lemma}
\begin{proof}
    Note that 
    \begin{displaymath}
        \lan X,Y\ran=\textup{trace}(-I_{p,q}XI_{p,q}Y)=\textup{trace}(-I_{p,q}YI_{p,q}X)=\lan Y,X\ran.
    \end{displaymath}
    Since $\overline{\lan X,Y\ran}=\lan Y,X\ran$ due to the invariance of trace under transpose, we conclude that the inner product is real and symmetric. It is positive-definite as $\lan X,X\ran=\textup{trace}(X^\ast X)\geq 0$ and equality holds if and only if $X$ is the zero matrix. 
\end{proof}

\hf The Riemannian metric obtained by left translations of $\lan\cdot,\cdot\ran$ will also be denoted by $\lan \cdot,\cdot\ran$. We shall analyze the geodesics for this metric. The Lie algebra $\mathfrak{u}_p\oplus \mathfrak{u}_q$ of $U(p)\times U(q)$ consists of 
\begin{displaymath}
    \left(
        \begin{array}{cc}
            A & 0 \\
            0 & D
        \end{array}\right),\,\,A+A^\ast=0,\,D+D^\ast =0.
\end{displaymath}
Let $\mathfrak{n}$ denote the orthogonal complement of $\mathfrak{u}_p\oplus \mathfrak{u}_q$ inside $\mathfrak{u}_{p,q}$. As $\mathfrak{n}$ is of (complex) dimension $pq$, and 
\begin{displaymath}
    \left\{\left(
        \begin{array}{cc}
            0 & B\\
            B^\ast & 0
        \end{array}\right)\,\Big|\,B\in M_{p,q}(\C)\right\}
\end{displaymath}
is contained in $\mathfrak{n}$, this is all of it. We may verify that
\begin{eqnarray*}
    \left[ \left(\begin{array}{cc}
    A & 0\\
    0 & D
    \end{array}\right),\left(\begin{array}{cc}
    0 & B\\
    B^\ast & 0
    \end{array}\right)\right] & = & \left(\begin{array}{cc}
    0 & AB-BD\\
    DB^\ast-B^\ast A & 0
    \end{array}\right)\in\mathfrak{n}\\
    \left[ \left(\begin{array}{cc}
    0 & B\\
    B^\ast & 0
    \end{array}\right),\left(\begin{array}{cc}
    0 & C\\
    C^\ast & 0
    \end{array}\right)\right] & = & \left(\begin{array}{cc}
    BC^\ast-CB^\ast & 0\\
    0 A & B^\ast C-C^\ast B
    \end{array}\right)\in\mathfrak{u}_p\oplus \mathfrak{u}_q.
\end{eqnarray*}
\begin{lemma}
    Let $\gamma$ be the integral curve, initialized at $e$, for a left-invariant vector field $Y$. This curve is a geodesic if $Y(e)$ either belongs to $\mathfrak{n}$ or to $\mathfrak{u}_p\oplus\mathfrak{u}_q$.
\end{lemma}
\begin{proof}
    The Levi-Civita connection $\nabla$ is given by the Koszul formula
    \begin{displaymath}
        2\lan X,\nabla_Z Y\ran = Z\lan X,Y\ran+Y\lan X,Z\ran -X\lan Y,Z\ran +\lan Z,[X,Y]\ran+\lan Y,[X,Z]\ran - \lan X,[Y,Z]\ran. 
    \end{displaymath}
    Putting $Z=Y$ and $X$, two left-invariant vector fields, in the above, we obtain
    \begin{displaymath}
        \lan X, \nabla_Y Y\ran = \lan Y, [X,Y]\ran.
    \end{displaymath}
    To prove our claim, it suffices to show that $\nabla_Y Y=0$, ie, $\lan Y, [X,Y]\ran=0$ for any $X$. Let us assume that $Y(e)\in\mathfrak{n}$. If $X(e)\in\mathfrak{n}$, then $[X(e),Y(e)]\in \mathfrak{u}_p\oplus\mathfrak{u}_q$, which implies that $\lan Y(e), [X(e),Y(e)]\ran=0$. If $X(e)\in \mathfrak{u}_p\oplus\mathfrak{u}_q$, then
    \begin{eqnarray*}
        \lan Y, [X,Y]\ran & = &  \lan\left(\begin{array}{cc}
        0 & B\\
        B^\ast & 0
        \end{array}\right), \left(\begin{array}{cc}
        0 & AB-BD\\
        DB^\ast-B^\ast A & 0
        \end{array}\right)\ran\\
        & = & \textup{trace}\left(\begin{array}{cc}
        B(DB^\ast-B^\ast A) & 0\\
        0 & B^\ast(AB-BD)
        \end{array}\right)\\
        & = & \textup{trace}(BDB^\ast-BB^\ast A)+\textup{trace}(B^\ast AB-B^\ast BD)\\
        & = & 0
    \end{eqnarray*}
    by the cyclic property of trace. Thus, $\nabla_Y Y=0$ if $Y(e)\in\mathfrak{n}$; similar proof works if $Y(e)\in \mathfrak{u}_p\oplus\mathfrak{u}_q$. 
\end{proof}

\begin{rem}
    An integral curve of a left-invariant vector field (also called $1$-parameter subgroups) need not be a geodesic in $\upq$. For instance, if $X+Y$ is a left-invariant vector field given by $X(e)\in\mathfrak{u}_p\oplus\mathfrak{u}_q$ and $Y(e)\in \mathfrak{n}$, then $\nabla_{X+Y}(X+Y)=0$ if and only if $\nabla_X Y=\frac{1}{2}[X,Y]$ and $\nabla_Y X=\frac{1}{2}[Y,X]$. This happens if and only if the metric is bi-invariant, i.e.,
    \begin{displaymath}
        \lan [X,Z],Y\ran=\lan X, [Z,Y]\ran.
    \end{displaymath} 
    This is not true in general; for instance, with $X(e)\in \mathfrak{u}_p\oplus\mathfrak{u}_q$ and linearly independent $Y(e),Z(e)\in \mathfrak{n}$, we get $\lan [X,Z],Y\ran-\lan X, [Z,Y]\ran\neq 0$.
\end{rem}

\hf Consider the matrix
\begin{displaymath}
    Y=\left(\begin{array}{cc}
    0 & B\\
    B^\ast & 0
    \end{array}\right)\in\mathfrak{n}.
\end{displaymath}
Let $B=U \sqrt{B^\ast B}$ and $B^\ast =\sqrt{B^\ast B}\,U^\ast$ be the polar decompositions for the rectangular matrices. It follows from direct computation that
\begin{eqnarray*}
    e^Y & = & \left(\begin{array}{cc}
    I_p+\frac{BB^\ast}{2!} + \frac{(BB^\ast)^2}{4!}+\cdots & \frac{B}{1!}+\frac{B (B^\ast  B)}{3!} + \frac{B (B^\ast B)^2 }{5!}+\cdots \\
    \frac{B^\ast}{1!}+\frac{(B^\ast B)B^\ast}{3!} + \frac{(B^\ast B)^2 B^\ast}{5!}+\cdots  & I_q+\frac{B^\ast B}{2!} + \frac{(B^\ast B)^2}{4!}+\cdots 
    \end{array}\right)\\
    & = & \left(\begin{array}{cc}
    \cosh(\sqrt{BB^\ast}) & U\sinh(\sqrt{B^\ast B})\\
    \sinh(\sqrt{B^\ast B})U^\ast & \cosh(\sqrt{B^\ast B})
    \end{array}\right).
\end{eqnarray*}
It can be checked that 
\begin{displaymath}
    e^\mathfrak{n}\cap \left(U(p)\times U(q)\right)=\{I_n\}.
\end{displaymath}
It is known that the non-zero eigenvalues of $Y$ are the non-zero eigenvalues of $\sqrt{BB^\ast}$ and their negatives. 
\begin{thm}\label{mainthm}
    For any element $\mathcal{A}\in \upq$, the associated matrix $\sqrt{\mathcal{A}^\ast\mathcal{A}}$ can be expressed uniquely as $e^Y$ for $Y\in \mathfrak{n}$. Moreover, there is a unique way to express $\mathcal{A}$ as a product of a unitary matrix and an element of $e^\mathfrak{n}$, and it is given by the polar decomposition.
\end{thm}

\vspace{0.3cm}
\noindent In order to prove the result, we discuss some preliminaries on logarithm of complex matrices. In general, there is no unique logarithm. However, the Gregory series \index{Gregory series}
\begin{displaymath}
    \log A=-\sum_{m=0}^{\infty}\frac{2}{2m+1}\left[(I-A)(I+A)^{-1}\right]^{2m+1}
\end{displaymath}
converges if all the eigenvalues of $A\in M_n(\C)$ have positive real part, see \cite[\S 11.3, page 273]{Hig08}. In particular, $\log A$ is well-defined for Hermitian positive-definite matrix. This is often called the \textit{principal logarithm}\index{principal logarithm} of $A$. This logarithm satisfies $e^{\log A}=A$. There is an integral form of logarithm that applies to matrices without real or zero eigenvalues; it is given by
\begin{displaymath}
    \log A =(A-I)\int_0^1 \left[s(A-I)+I\right]^{-1}ds.
\end{displaymath}
\begin{lemma}\label{inv}
    The inverse of $\mathcal{A}^\ast\mathcal{A}+I_n$ for $\mathcal{A}\in \upq$ is given by
    \begin{displaymath}
        \left[\mathcal{A}^\ast\mathcal{A}+I_n\right]^{-1}=\frac{1}{2}\left(\begin{array}{cc}
        I_p & -A^{-1}B\\
        -B^\ast (A^\ast)^{-1} & I_q
        \end{array}\right).
    \end{displaymath}
\end{lemma}
\begin{proof}
    Since $\mathcal{A}^\ast\mathcal{A}$ has only positive eigenvalues, $\mathcal{A}^\ast\mathcal{A}+I_n$ has no kernel. We note that 
    \begin{displaymath}
        \mathcal{A}^\ast\mathcal{A}+I_n=\left(\begin{array}{cc}
        2C^\ast C+2I_p & 2A^\ast B\\
        2B^\ast A & 2B^\ast B+2I_q
        \end{array}\right)=\left(\begin{array}{cc}
        2A^\ast A & 2A^\ast B\\
        2B^\ast A & 2D^\ast D
        \end{array}\right).
    \end{displaymath}
    The inverse matrix satisfies 
    \begin{displaymath}
        \left(\begin{array}{cc}
        2A^\ast A & 2A^\ast B\\
        2B^\ast A & 2D^\ast D
        \end{array}\right)\left(\begin{array}{cc}
        E & F\\
        F^\ast & G
        \end{array}\right)=\left(\begin{array}{cc}
        I_p & 0\\
        0 & I_q
        \end{array}\right).
    \end{displaymath} 
    As the matrices are Hermitian, the three constraints that $E,F,G$ must satisfy (and are uniquely determined by) are
    \begin{eqnarray*}
        E & = & \textstyle{\frac{1}{2}}(A^\ast A)^{-1}-A^{-1}BF^\ast\\
        G & = & \textstyle{\frac{1}{2}}(D^\ast D)^{-1}-D^{-1}CF\\
        F & = & -A^{-1}BG.
    \end{eqnarray*}
    We note that $E=\frac{1}{2}I_p$, $G=\frac{1}{2}I_q$ and $F=-\frac{1}{2}A^{-1}B$ satisfy the above equations. For instance, 
    \begin{align*}
        \frac{1}{2}(A^\ast A)^{-1}-A^{-1}BF^\ast & = \frac{1}{2}(A^\ast A)^{-1}+\frac{1}{2}A^{-1}BB^\ast (A^{\ast})^{-1} \\
        & = \frac{1}{2}(A^\ast A)^{-1}+\frac{1}{2}A^{-1}(AA^\ast-I_p)(A^{\ast})^{-1}=\frac{1}{2}I_p,
    \end{align*}
    where $BB^\ast=AA^\ast-I_p$ is a consequence of $\mathcal{A}^\ast\in \upq$. Yet another consequence is $AC^\ast=BD^\ast$, which is equivalent to
    \begin{displaymath}
        A^{-1}B=(D^{-1}C)^\ast.
    \end{displaymath}
    In a similar vein,
    \begin{align*}
        \frac{1}{2}(D^\ast D)^{-1}-D^{-1}CF & = =\frac{1}{2}(D^\ast D)^{-1}+\frac{1}{2}D^{-1}CC^\ast (D^{\ast})^{-1} \\
        & = \frac{1}{2}(D^\ast D)^{-1}+\frac{1}{2}D^{-1}(DD^\ast-I_q)(D^{\ast})^{-1} \\
        & = \frac{1}{2}I_q,
    \end{align*}
    where $CC^\ast=DD^\ast-I_q$ is due to $\mathcal{A}^\ast\in \upq$.
\end{proof}

\begin{proof}[Proof of \Cref{mainthm}]
    We use Gregory series expansion for computing the principal logarithm of $\mathcal{A}^\ast\mathcal{A}$ along with \Cref{inv}:
    \begin{align*}
        & \log (\mathcal{A}^\ast\mathcal{A}) 
        \\
        & = \sum_{m=0}^\infty{\frac{2}{2m+1}}\left[2\left(\begin{array}{cc}
        A^\ast A-I_p & A^\ast B\\
        B^\ast A & D^\ast D-I_q
        \end{array}\right)
        \frac{1}{2}\left(\begin{array}{cc}
        I_p & -A^{-1}B\\
        -B^\ast (A^\ast)^{-1} & I_q
        \end{array}\right)\right]^{2m+1}\\
        & = \sum_{m=0}^\infty{\frac{2}{2m+1}}\left(\begin{array}{cc}
        0 & A^{-1}B\\
        B^\ast (A^\ast)^{-1} & 0
        \end{array}\right)^{2m+1}.
    \end{align*}
    We set $Y=\frac{1}{2}\log (\mathcal{A}^\ast\mathcal{A})$. It is clear that $Y\in\mathfrak{n}$ and $e^Y=\sqrt{\mathcal{A}^\ast\mathcal{A}}$. It is known that the exponential map is injective on Hermitian matrices. This implies the uniqueness of $Y$.
    
    \vspace{0.1cm}
    \hf If $U_1e^{Y_1}=U_2 e^{Y_2}$ are two decompositions of $\mathcal{A}\in\upq$ with $U_i\in \textup{U}(p)\times\textup{U}(q)$ and $Y_i\in\mathfrak{n}$, then 
    \begin{displaymath}
        e^{2Y_1}=e^{Y_1}U_1^\ast U_1 e^{Y_1}=e^{Y_2}U_2^\ast U_2 e^{Y_2}=e^{2Y_2}.
    \end{displaymath}
    By the injectivity of the exponential map (on Hermitian matrices), we obtain $Y_1=Y_2$, which implies that $U_1=U_2$.
\end{proof}

\hf We infer the following (see \cite[ Lemma 1, page 211]{YaSt75} for a different proof) result.
\begin{cor}\label{expsurj}
    The exponential map $\textup{exp}:\mathfrak{u}_{p,q}\to U(p,q)$ is surjective.
\end{cor}
\begin{proof}
    Using the polar decomposition and \Cref{mainthm}, 
    \begin{displaymath}
        \mathcal{A}=\mathcal{A}\big(\sqrt{\mathcal{A}^\ast\mathcal{A}}\big)^{-1}\sqrt{\mathcal{A}^\ast\mathcal{A}}=\mathcal{A}\big(\sqrt{\mathcal{A}^\ast\mathcal{A}}\big)^{-1}e^Y.
    \end{displaymath}
    Since the matrix exponential is surjective for $U(p)\times U(q)$, choose $Z\in \mathfrak{u}_p\oplus\mathfrak{u}_q$ such that $e^Z=\mathcal{A}(\sqrt{\mathcal{A}^\ast\mathcal{A}})^{-1}$. By Baker-Campbell-Hausdorff formula, we may express $e^Z e^Y$ as exponential of an element in $\mathfrak{u}_{p,q}$. 
\end{proof}

\hf The distance from any matrix $\mathcal{A}\in \upq$ to $U(p)\times U(q)$ is given by the length of the curve
\begin{displaymath}
    \gamma(t)=\mathcal{A}\big(\sqrt{\mathcal{A}^\ast\mathcal{A}}\big)^{-1}e^{tY},
\end{displaymath}
which can be computed (and simplified via left-invariance) as follows
\begin{displaymath}
    \ell(\gamma)=\int_0^1 \|\gamma'(t)\|_{\gamma(t)}\,dt=\int_0^1 \|Y\|\,dt=\|Y\|.
\end{displaymath}
Note that 
\begin{displaymath}
    \|Y\|^2=\textup{trace}(Y^\ast Y)=\textup{trace}\left[\textstyle{\frac{1}{4}}(\log (\mathcal{A}^\ast\mathcal{A}))^2\right].
\end{displaymath}
Thus, the distance squared function is given by 
\begin{displaymath}
    d^2:\upq\to\R,\,\,\mathcal{A}\mapsto \textstyle{\frac{1}{4}}\textup{trace}\left[\left(\log (\mathcal{A}^\ast\mathcal{A})\right)^2\right].
\end{displaymath}